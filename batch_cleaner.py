import pandas as pd #Tri·ªáu h·ªìi" th∆∞ vi·ªán Pandas. T∆∞·ªüng t∆∞·ª£ng n√≥ l√† m·ªôt chuy√™n gia l√†m vi·ªác v·ªõi file Excel/CSV. ƒê·ªçc, x√≥a, s·ª≠a file CSV l√† nh·ªù n√≥ h·∫øt.
import re 
import json
import os #Gi√∫p script c·ªßa m√¨nh "nh√¨n" ƒë∆∞·ª£c c√¢y th∆∞ m·ª•c, t√¨m file, l·∫•y ƒë∆∞·ªùng d·∫´n.
import argparse
from tqdm import tqdm #t·∫°o ra thanh loading %
import spacy

#T·∫¢I MODEL AI (spaCy)
try:
    nlp = spacy.load("en_core_web_sm") #T·∫£i model AI nh·ªè g·ªçn c·ªßa spaCy ƒë·ªÉ x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP)
    print("[INFO] ƒê√£ t·∫£i xong model AI (spaCy). B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...") 
except OSError:
    print("[L·ªñI] Kh√¥ng t√¨m th·∫•y model 'en_core_web_sm' c·ªßa spaCy.") #C√°i try...except l√† ƒë·ªÉ ph√≤ng h·ªù: N·∫øu m√†y ch∆∞a t·∫£i b·ªô n√£o n√†y (l·ªói OSError), th√¨ b√°o l·ªói v√† ch·ªâ c√°ch t·∫£i.
    print("Vui l√≤ng ch·∫°y 2 l·ªánh sau trong terminal:")
    print("1. pip install spacy")
    print("2. python -m spacy download en_core_web_sm")
    exit(1)


# C√°c t·ª´ kh√≥a ƒë·ªÉ ph√¢n bi·ªát Tr∆∞·ªùng h·ªçc v√† C√¥ng ty
# Ch√∫ng ta d√πng danh s√°ch n√†y ƒë·ªÉ ph√¢n lo·∫°i
EDUCATION_KEYWORDS = [
    'university', 'college', 'institute', 'academy', 'school',
    'ƒë·∫°i h·ªçc', 'h·ªçc vi·ªán', 'tr∆∞·ªùng', 'c·ª≠ nh√¢n', 'th·∫°c sƒ©',
    'bachelor', 'master'
]

def extract_education(about_text): #nh·∫≠n 1 ƒëo·∫°n vƒÉn b·∫£n "about_text"
    """
    Tr√≠ch xu·∫•t H·ªåC V·∫§N b·∫±ng AI (spaCy).   
    Ch√∫ng ta t√¨m c√°c 'T·ªï ch·ª©c' (ORG) c√≥ ch·ª©a t·ª´ kh√≥a gi√°o d·ª•c.
    """
    if not isinstance(about_text, str):
        return None
    
    doc = nlp(about_text)
    matches = []
    
    # 1. T√¨m b·∫±ng c·∫•p (Bachelor, Master)
    # D√πng regex ƒë∆°n gi·∫£n cho c√°c b·∫±ng c·∫•p ph·ªï bi·∫øn
    try:
        degrees = re.findall(
            r'(Bachelor of [^\n\.|,]+|Master of [^\n\.|,]+|C·ª≠ nh√¢n [^\n\.|,]+|Th·∫°c sƒ© [^\n\.|,]+)', 
            about_text, 
            re.IGNORECASE
        )
        if degrees:
            matches.extend([d.strip() for d in degrees])
    except Exception:
        pass # B·ªè qua n·∫øu regex l·ªói
        
    # 2. T√¨m t√™n tr∆∞·ªùng b·∫±ng AI
    for ent in doc.ents:
        # N·∫øu AI t√¨m th·∫•y m·ªôt 'T·ªï ch·ª©c' (ORG)
        if ent.label_ == 'ORG':
            # Ki·ªÉm tra xem t√™n t·ªï ch·ª©c ƒë√≥ c√≥ ch·ª©a t·ª´ kh√≥a gi√°o d·ª•c kh√¥ng
            text_lower = ent.text.lower()
            if any(keyword in text_lower for keyword in EDUCATION_KEYWORDS):
                matches.append(ent.text.strip())
                
    if not matches:
        return None
        
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ duy nh·∫•t, kh√¥ng tr√πng l·∫∑p
    return "; ".join(sorted(list(set(matches)), key=matches.index))

def extract_experience(about_text):
    """
    Tr√≠ch xu·∫•t KINH NGHI·ªÜM b·∫±ng AI (spaCy).
    Ch√∫ng ta t√¨m c√°c 'T·ªï ch·ª©c' (ORG) KH√îNG ch·ª©a t·ª´ kh√≥a gi√°o d·ª•c.
    """
    if not isinstance(about_text, str):
        return None

    doc = nlp(about_text)
    matches = []
    
    # 1. T√¨m (Title @ Company) ho·∫∑c (Title t·∫°i Company) - Regex c≈© v·∫´n t·ªët
    try:
        # T√¨m "Title" @ "Company"
        matches_at = re.findall(r'([^\n\|@]{5,70})\s*@\s*([^\n\|@\.]{3,70})', about_text)
        if matches_at:
            matches.extend([f"{title.strip()} @ {company.strip()}" for title, company in matches_at if '.' not in company])
            
        # T√¨m (Title t·∫°i Company)
        matches_tai = re.findall(
            r'([a-zA-Z\s]+(?:Engineer|Developer|Analyst|Manager|Scientist))\s+t·∫°i\s+([^\n\.]+)',
            about_text, re.IGNORECASE
        )
        if matches_tai:
            matches.extend([f"{title.strip()} t·∫°i {company.strip()}" for title, company in matches_tai])
    except Exception:
        pass
        
    # 2. T√¨m t√™n C√¥ng ty b·∫±ng AI
    for ent in doc.ents:
        # N·∫øu AI t√¨m th·∫•y m·ªôt 'T·ªï ch·ª©c' (ORG)
        if ent.label_ == 'ORG':
            # Ki·ªÉm tra xem n√≥ KH√îNG PH·∫¢I l√† tr∆∞·ªùng h·ªçc
            text_lower = ent.text.lower()
            if not any(keyword in text_lower for keyword in EDUCATION_KEYWORDS):
                # V√† lo·∫°i b·ªè c√°c t√™n qu√° ng·∫Øn (c√≥ th·ªÉ l√† r√°c)
                if len(ent.text.strip()) > 3:
                    matches.append(ent.text.strip())

    if not matches:
        return None
        
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ duy nh·∫•t, kh√¥ng tr√πng l·∫∑p
    return "; ".join(sorted(list(set(matches)), key=matches.index))


# --- H√ÄM X·ª¨ L√ù (M·ªñI FILE) ---
# (H√†m n√†y gi·ªØ nguy√™n nh∆∞ c≈©, ch·ªâ thay ƒë·ªïi t√™n c·ªôt)
def process_single_file(input_csv_path, output_json_path):
    """
    Quy tr√¨nh: Load CSV -> Clean -> Extract -> Save JSON
    """
    try:
        if not os.path.exists(input_csv_path):
            return False
            
        df = pd.read_csv(input_csv_path)
    except Exception as e:
        print(f"  [L·ªñI] Kh√¥ng th·ªÉ ƒë·ªçc file CSV: {e}")
        return False

    # --- B∆∞·ªõc 1: L√†m s·∫°ch c∆° b·∫£n ---
    # T√™n c·ªôt ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t theo file Excel c·ªßa b·∫°n
    url_column = 'URL'     
    name_column = 'Name'   
    about_column = 'About' 
    
    if url_column not in df.columns or name_column not in df.columns or about_column not in df.columns:
        print(f"  [L·ªñI] Thi·∫øu c√°c c·ªôt (URL, Name, About) trong file: {input_csv_path}")
        return False

    df = df.drop_duplicates(subset=[url_column], keep='first')
    df = df.dropna(subset=[url_column, name_column])

    # --- B∆∞·ªõc 2: Tr√≠ch xu·∫•t th√¥ng tin ---
    # √Åp d·ª•ng c√°c h√†m tr√≠ch xu·∫•t AI M·ªöI
    df['education_extracted'] = df[about_column].apply(extract_education)
    df['experience_extracted'] = df[about_column].apply(extract_experience)
    
    # --- B∆∞·ªõc 3: L∆∞u sang JSON ---
    try:
        data_json = df.to_json(orient='records', indent=4, force_ascii=False)
        with open(output_json_path, 'w', encoding='utf-8') as f:
            f.write(data_json)
        return True
    except Exception as e:
        print(f"  [L·ªñI] Kh√¥ng th·ªÉ l∆∞u file JSON: {e}")
        return False

# --- H√ÄM CH√çNH (QU√âT TH∆Ø M·ª§C) ---
# (H√†m n√†y gi·ªØ nguy√™n nh∆∞ c≈©)
def batch_process_all(base_directory):
    print(f"üöÄ B·∫Øt ƒë·∫ßu qu√©t h√†ng lo·∫°t t·ª´ th∆∞ m·ª•c: {base_directory}")
    print("=" * 60)
    
    try:
        all_industries = [d for d in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, d))]
    except FileNotFoundError:
        print(f"[L·ªñI] Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {base_directory}")
        return
        
    if not all_industries:
        print("[L·ªñI] Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ng√†nh ngh·ªÅ n√†o trong 'data_collected'.")
        return

    success_count = 0
    fail_count = 0

    for industry_name in tqdm(all_industries, desc="X·ª≠ l√Ω c√°c ng√†nh", unit="folder"):
        industry_path = os.path.join(base_directory, industry_name)
        input_csv = os.path.join(industry_path, "profiles.csv")
        output_json = os.path.join(industry_path, f"cleaned_profiles.json")
        
        if process_single_file(input_csv, output_json):
            success_count += 1
        else:
            fail_count += 1
            
    print("\n" + "=" * 60)
    print("‚úÖ X·ª≠ l√Ω h√†ng lo·∫°t ho√†n t·∫•t!")
    print(f"  - {success_count} th∆∞ m·ª•c ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng.")
    print(f"  - {fail_count} th∆∞ m·ª•c b·ªã b·ªè qua (l·ªói ho·∫∑c thi·∫øu file 'profiles.csv').")
    print("=" * 60)

# --- ƒêI·ªÇM V√ÄO SCRIPT ---
# (Gi·ªØ nguy√™n nh∆∞ c≈©)
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="L√†m s·∫°ch H√ÄNG LO·∫†T v√† chuy·ªÉn ƒë·ªïi CSV sang JSON (Phi√™n b·∫£n AI).")
    parser.add_argument('--dir', type=str, default='data_collected', help="Th∆∞ m·ª•c c∆° s·ªü (m·∫∑c ƒë·ªãnh: 'data_collected')")
    args = parser.parse_args()
    
    try:
        import pandas as pd
        from tqdm import tqdm
    except ImportError:
        print("[L·ªñI] Thi·∫øu th∆∞ vi·ªán. Vui l√≤ng ch·∫°y: pip install pandas tqdm")
        exit(1)
        
    batch_process_all(args.dir)